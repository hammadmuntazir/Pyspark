{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb627e10-bc58-41fc-b220-205c888511b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Create a Dataframe using a connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72be9634-7f8f-4e91-be95-ed80cb7e94b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(path=\"/Volumes/dev/spark_db/datasets/spark_programming/data/sf-fire-calls.csv\")\n",
    ")\n",
    "# read method give access to spark connector\n",
    "#there are more than 100 connector availables and we use them by format method\n",
    "# in format we tell which connector i.e. csv here . in databricks csv connector is preinstalled\n",
    "# it will read csv file using the CSV connector\n",
    "# when header equal to true, it means i am telling that connector the first line in file is header line you will      find coulmn names there it will not includes\n",
    "# inferschema mean please read data and make guess of data type of each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048bba5b-be88-4586-884c-d18621f8d4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_file_df = (\n",
    "    spark.read.format(\"json\")\n",
    "         .load(path=\"/Volumes/dev/spark_db/datasets/spark_programming/data/diamonds.json\")\n",
    ")\n",
    "# json format by default infer schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7372e9c5-aa59-445a-a877-16976e812e54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Create a Dataframe reading a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27442a2d-97b9-4da7-83e0-b159fa7e0842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_df = spark.table(\"dev.spark_db.sf_fire_calls\")\n",
    "# if we have data in spark table then there is no need of any connector\n",
    "# table api will its self read data from table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d03b550-9094-4385-8eb5-1a76abdaff06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Create a Dataframe reading the result of a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc0cb6c-c2bd-4dda-9ff5-8637704b31e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sql_df1=spark.sql(\"select *  from dev.spark_db.sf_fire_calls\")\n",
    "# let's i donot want to read all data from table i will read only 5 rows from table\n",
    "table_name  = \"dev.spark_db.sf_fire_calls\"\n",
    "sql_df = spark.sql(f\"\"\"select * \n",
    "                   from {table_name} \n",
    "                   limit 5\"\"\")\n",
    "sql_df=spark.sql(\"select CallNumber,CallType from dev.spark_db.sf_fire_calls limit 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0536fe-a467-450a-ac2e-1cfacf9cf97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I am writing a parameterized query let's suppose table_name is parameter\n",
    "table_name  = \"dev.spark_db.sf_fire_calls\"\n",
    "sql_df = spark.sql(f\"\"\"select * \n",
    "                   from {table_name} \n",
    "                   limit 5\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52621f4-0245-46af-9853-d3ea4e1c7ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4. Create a dataframe from a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6af060-509c-4a7c-a77b-e3b8b0339384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#this approach is mainly used for testing ,testing our spark _code where we want to test data in python list\n",
    "# common approach for creating test data frame\n",
    "from datetime import datetime, date\n",
    "\n",
    "data_list_schema = 'id long, name string, joining_date date, salary double, created_at timestamp'\n",
    "data_list = [(1, \"Prashant\", date(2018, 1, 1), 924.0, datetime(2022, 1, 1, 9, 0)),\n",
    "             (2, \"Sushant\", date(201, 2, 1), 1260.50, datetime(2022, 1, 2, 11, 0)),\n",
    "             (3, \"David\", date(2022, 3, 1), 765.0, datetime(2022, 1, 3, 10, 00))]\n",
    "\n",
    "list_df = spark.createDataFrame(data_list, data_list_schema) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3beb10-4065-461a-a954-463ed1adbd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. Create a single column dataframe from a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adc91f1-bf64-485d-b454-91880aff88c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "range_df=spark.range(1000,1010,2)\n",
    "range_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-creating-dataframe",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
